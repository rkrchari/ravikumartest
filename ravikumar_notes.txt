

Dataset -- house prices in 5 areas of Blore for the years 2018 & 2019.

problem statement -- to build a predictive machine learning model that could
predict the house prices in Bangalore.


Location, Builder's Name, Sq.Ft, No. Of Bedrooms, No. Of Bathrooms, Balcony,
Car Parking facility,  Club House , House Price 

Factors / features / independent variables 


Outcome / target / Dependent variable / label 
==============================================================================

Supervised LEarning --> Classification & REgression

REgression 

We opt for the regression suprevised ML technique when the target variable
is of continuous numeric type.

Eg: House Price -- 20 lakh rupees , 33,33,500 rupees as well, 3 crore rupees 
                   as well

Eg : Weather forecast , stock prices, employee salary , health vitals,
     product prices etc., 
============================================================================
Regression:
1. Linear Regression --> These algorithms are used when the spread of dataset
is a linear one.

2.Non-linear Regression --> These algorithms are used when the spread of dataset
is a non-linear one.

Linear Regression:

1. Simple linear regression 

2. Multiple linear regression 


Regression -> statistical technique wherein the strength of reltionship between
              different variables ( between the target and the features) is
              figured out.

1. Simple linear regression 

    y = mx +c  , y--> house price , x--> sq.ft of the house 

y --> target variable , m --> regression co-efficient , c --> intercept 

1 target variable  & only 1 feature, the model that you built for this
data is called a simple linear regression model.
=============================================================================

2. Multiple Linear Regression 

1 target variable and when you have more than 1 feature ==> MLR

    y = b1x1 + b2x2 + b3x3 + b4x4 + ....... + bnxn + c


y -- > target variable , x1, x2, x3, x4 ..xn --> the features present in the
       dataset, c - intercept , b1, b2 , b3 ...bn --> regression co-efficients
       where b1 -- represents the strength of relationship between x1 & y
       where b2 -- represents the strength of relationship between x2 & y


=============================================================================

Steps followed in building a machine learning model:

Once EDA(Exploratory Data Analysis) is done, 

1. Identify the target variable in your dataset.

2. Separate the features(X) and the target variable(y)

3. Separate our dataset into train and test datasets --split ratio -- 75 :25,
80 :20 , 70 :30 , 60 :40 

4. Building the ML model :
   (i)  Instantiate the estimator object 

        model = LinearRegression() , the "model" variable is called estimator object.

   (ii) Build our model with training dataset

   (iii) Validate our model using the testing dataset with the help of 
         suitable evaluation metrics
  
=============================================================================
Evaluate a regression model :

RMSE - root mean square error -- can range from 0 to infinity.

R2_score - how well our model fit the dataset-- ranges from 0 to 1. 

==============================================================================

Correlation in regression:

--> how two variables are co-related with each other 

positively correlated , negatively correlated , unrelated 


 y = b1x1 + b2x2 + b3x3 + b4x4 + .... + bnxn + c

Location, Builder's Name, Sq.Ft, No. Of Bedrooms, No. Of Bathrooms, Balcony,
Car Parking facility,  Club House , House Price 

 y --> house price , x1 -- location , x2 - builders name , x3 - sq. ft 
 x4 - no of bedrooms , x5 -- no of bathrooms .... 


regression - strength of relationship between 2 variables 

assumption -- all the features in the dataset are unrelated or they are independen
              of each other. 

for finding out the relation between x1 and y , the model will consider that
changes done in the values of x1 will affect only y and not any other feature.

how do we figure out correlation in our dataset ?

either use a heatmap or u cud just simply see the correlation values with 
the help of pandas corr() function.


correlation values -> (-1,+1)

positive : when one value increases / decreases , the other value also increases
           or decreases respectively. (0,+1)

negative correlation : when one value increases and the other decreases.

one of the features in the feature pairs which are either strongly positively
correlated or strongly negatively correlated are removed 

(0.8,0.9,1) - strong positive correlation values
(-.8,-.9,-1) - strong negative correlation values 
=============================================================================



Eg: employee dataset 

Salary             : 10K -- 4Lakh / month 

Age of the employee: 22 - 60 

Years of experience: 0 - 40

=============================================================================
Polynomial regression :

https://towardsdatascience.com/polynomial-regression-bbe8b9d97491

==============================================================================


 




Overfitting :

When the training accuracy > testing accuracy 

rmse value , training rmse value < testing rmse 
----------------------------------------------------------------------
Underfitting :

When the training accuracy itself is very low. 

rmse value , training rmse value itself would be very high 
----------------------------------------------------------------------
Right - fit / correct -fit 

When there is no significant difference between the training and testing 
accuracy / rmse value
------------------------------------------------------------------------

Ridge :

Ridge regression is best used when there are high degrees of collinearity
or nearly linear relationships in the set of features.


Regression equation --> y = b1x1 + b2x2 + c


Ridge regression --> b1x1+b2x2 +c + (lambda) * (b1^2 + b2^2)

==============================================================================
Correlation in regression :

co- relation 

house price data:

no.of bedrooms, no.of bathrooms, sq ft, balcony presence , builders name,
area name, the number of pine trees in the apartment, house price 

total -> 7 

6 -> features , 1 -> target

house price(y)  = (b1)*no.ofbedrooms + (b2)*no.of bathrooms + (b3) * sq ft +..

std assumption : the features are independent of each other.(i.e) there is no 
                 correlation between them.
                 The target variable is dependent on the features.



features are also called independent variables
target is called the dependent variable 

==> no .of bedrooms feature and the sq ft feature are correlated --+ve correlation

--> types of correlation -- positive & negative 


positive correlation : when one value increases/decreases with an increase/decrease
                       in the other  value respectively.
                      

negative correlation: when one value increases with a decrease in the other
                      value & vice versa 

correlation -> (-1,1)

 + ve correlation -- 0 to 0.6 -- permissible but >= 0.7 is not permissible bcoz this implies strong positive correlation
 - ve correlation -- -0.6 to 0 -- premissible ,but <= -0.7 is not permissible bcoz this implies strong negative correlation




multicollinearity arises when the independent variables in a regression model
are correlated.

problems of multicollinearity :

1. it will affect the coefficient estimates such that these co-efficients 
become sensitive to very small changes as well.

2. reduces the prediction accuracy(i.e) it will affect the RMSE of our regression model.

========================================================================================================

Lasso : 

implicitly the model features are selected.


house price(y)  = (b1)*no.ofbedrooms + (b2)*no.of bathrooms + (b3) * balcony +
                     + (b4) * no of pine trees 

lasso sets the co efficient values of the non significant features as a low 
value or potentially 0.

--> feature selection happens in lasso.


Classification :

Supervised ML approach . when the target variable(y) is of categorical type,
then you would choose classification. 

Ex : Tall/Short/Medium , Sick/healthy , Grades of a student(A/B/C/D/E/F)

algorithms:

1. Logistic Regression
2. K Nearest Neighbors 
3. Decision Trees
4. Support Vector Machines
5. Naive Bayes Classifier 

===========================================================================

Quantitative variables , Qualitative variables 

Quantitative -- continuous - Eg: stock price , discrete - Eg: Population, no of houses

Qualitative -- nominal , ordinal , binomial(binary)


binomial -- 0/1 -- True/False
nominal  -- fruit names, gender(male/female/transgender) 
      - when there are more than 2 categories and when there is no order between them
ordinal  -- grades of an exam - A>B>C>D>E>F , 5>4>3>2>1
======================================================================================

Encoding:

--the process of converting textual data to numbers

1.One hot encoding
2. Label encoding

One hot encoding :

Gender -- Male/Female/Transgender 

result :

          Gender_Male   | Gender_Female  |  Gender_Transgender

Male           1                 0                 0

Female         0                 1                 0

Transgender    0                 0                 1


pd.get_dummies(df['Gender'])

OHE is used to encode all the features only.

Location column --> Area A, Area b , Area C , Area D , Area E

apply OHE on the Location column (i.e) pd.get_dummies(df['Location'])

           Location_AreaA | Location_AreaB | Location_AreaC | Location_AreaD | Location_Area5
-----------------------------------------------------------------------------

Label encoding :

Grades -- A B C D E F

result:
Grades  -- 0 1 2 3 4 5 (these numbers are assigned according to the alphabetical order)

Label encoding should be used for encoding the categorical target variables always.

Why only LE and not OHE for target variable?

Bcoz the target variable has to remain as a single column

apply label encoder on Location column --> Area A, Area b , Area C , Area D , Area E

Area A - 1 , Area B - 2 , Area C - 3, Area D - 4 , Area E -5 --> introduce
ordinality in a nominal categorical feature. This is the drawback of using
Label encoder on the nominal categorical feature. 

To apply label encoder on a column, we need to import the separate package from
scikit learn Library. 
----------------------------------------------------------------------------
Classification Evaluation Metrics :

For a 2 class classification problem , we consider confusion matrix          


Cancer / not 

                       cancer |    not       (predicted value)

(actual value) cancer|  10(TP)|     20 (FN) 

               not   |  30(FP)|     40 (TN)


here, we have considered 100 data points 

the model has correctly predicted the people who have cancer , as cancerous
and this number = 10 = True Positives 

the model has correctly predicted the people who are healthy , as healthy
and this number = 40  = True negatives 

The model has wrongly predicted the people who are not having cancer as 
cancerous and this number = 30 = False Positive

The model has wrongly predicted the people who are having cancer as healthy
and this number = 20 = False Negative 

False Positive -- are also called Type 1 error

False Negative -- are also called Type 2 error

Type 1 error is less harmful than a type 2 error. 

Based on the values in the confusion matrix , we have 4 different metrics 
for evaluating a classification model.

1. Accuracy  = TP + TN / (TP + TN + FP + FN)

2. Precision = TP / (TP + FP ) -- how precisely your model is identifying the
               cancerous patients ( positive class)
             = TN / (TN + FN) -- how precisely your model is identifying the
               healthy people(negative class)
              the other name of precision metric is specificity

3. Recall   = TP / TP + FN(positive)  or TN/ TN+ FP. the other name for the 
              recall metric is sensitivity

4. F1 Score = 2 * Recall*Precision / Recall + Precision

=============================================================================

scikit learn metrics -- accuracy_score --> tells you how accurate your model 
                                           is

scikit learn metrics -- classification report --> tells you about the precision,
                                                recall and f1 score. 

==============================================================================

Example for understanding scaling / normalizing the data:

Eg: Employee dataset - about 2000 employees of a company 

Salary/month -- Rs.15,000/- - Rs.5,00,000/-
Age          -- 21 - 60
Years of Exp -- 0  - 40 

target -- whether a person will stay or leave the company?

result of normalization/ standadisation of data features:

Salary / month -- 0 to 1
Age            -- 0 to 1 
Years of Exp   -- 0 to 1 

Different scalers are available -- MinMax Scaler, Standard Scaler 
Scaling is necessary whenever there are a number of numerical type feature
in the dataset.
-----------------------------------------------------------------------------

Identifying the quality of the ML model:

1. Underfitting --> The ML model performs extremely poor on both the training dataset 
and the testing dataset

(i.e) Training accuracy < less than optimal limits 
Testing accuracy <  less than optimal limits 

==> this demands a change in the ML algorithm beign used 

2. Appropriate -->The ML model performs well on both the training dataset 
and on the testing dataset
(i.e) Training accuracy  == expected value
Testing accuracy == expected value


3. Overfitting ---> The ML model performs extremely well on the training dataset 
while it fails miserably on the testing dataset

(i.e) Training accuracy > Testing accuracy 

-------------------------------------------------------------------------------------


Ensemble learning:

-- It is a technique used for improving prediction accuracy.
-- It uses multiple learning algorithms instead of a single algorithm.


-- 2 commonly used ensemble methods are:

-- Bagging 
-- Boosting 

Bagging --> 

Eg : Random Forest Classifier
-- A bagging technique based on decision tree algorithm.

-- n number of decision trees put together -- > forest.
-- The input data to the algorithm is sub sampled ( subset of both the
records and columns) and then it is sent to every decision tree initialised
earlier at a random fashion. --> random

---> Random Forest Classifier.

data : 100 records, 10 features 
no. of DT - 5 

DT 1 -- > 20, 3 (1-20 records , 1-3 features)
DT 2 --> 30, 4 (21-50 records , 4-7 features)
DT 3 --> 40, 2(51-90 records, 9,10 features)
DT 4 --> 30, 5 (91-100, 1-20 records, 3,4,5,6,7 features)
DT 5 --> 50, 2 (31-80 records, 2,8 features)

Bagging --> 
Multiple models are trained using the same algorithm with a 
subset of the training data which is selected with replacement and is 
assigned to each training model present.

Bagging usually considers homogenous weak learners. It learns from them 
independently and then it combines the output of the weak learners in 
a deterministic averaging process.

Eg: Iris data 

3 classes - setosa, versicolor and virginica

RFC : 

n = 100 , it means that 100 decision tree models are initialised within the
          Random forest classifier algorithm.
45 trees -setosa , 30 - virginica , 25 - versicolor

new data point --> each and every decision tree on looking at the new
                   data point would actually vote for one of the target 
                   classes. The majority class label will be considered
                   as the output. 

Advantage of bagging:

The ensemble model based in bagging reduces the variance of the 
estimate(target column prediction value).

------------------------------------------------------------------------------
Boosting: 

Advantage -- it reduces bias and in turn increases prediction accuracy.

Boosting considers homogenous weak learners and it learns from them in a 
sequential manner. Unlike bagging, where the learning is parallel.
Here, the learning is also adaptive -- the base model depends on the 
previous model's output.



GBM - Many models are trained in a gradual, additive and sequential manner.

Adaboost identifies the shortcomings by adding a high weight for the 
misclassified data points. 
GBM -- it uses a gradient in the loss function to identify the misclassified
points. 

Loss function -- a measure that indicates how well a model's coefficients
are at fitting the underlying data. 
y = ax + b + e(error component) 
It combines all the outputs in a deterministic strategy.

----------------------------------------------------------------------

References:

https://xgboost.readthedocs.io/en/latest/
------------------------------------------------------------------------



Dimensionality reduction :

It is a technique used in ML to reduce the number of dimensions such that 
it retains only those most important components.


Advantages:

1. Reduces computational complexity
2. Reduces overfitting
3. Helps in visualizing by reducing the number of high dimensions.


--Dimensionality reduction is used both in supervised and unsupervised 
learning techniques.

PCA can be used for both supervised and unsupervised learning techniques
LDA can be used only for supervised learning technique
-----------------------------------------------------------------------------
PCA --

-- PCA preserves the correlation between features 
-- The principal components in PCA are created by linear combination of 
original variables.(Calculated with concepts like eigen values)
-- The principal components are orthogonal to each other.
-- The first principal component represents the direction of maximum variance.
-- PCA always performs well in a normalized dataset. 
-- The number of Principal Components generated is equal to or lesser than the number
of features present in the dataset.

Iris dataset --
Setosa , Versicolor and Virginica.

5 features --> 2 features 

sepal length and sepal width --> highest variance --> PC1 -- > PCA

petal length + sepal length --> output class labels are being classified
properly --> LDA

-----------------------------------------------------------------------------
LDA -- Linear Discriminant Analysis 

LDA tries to reduce the dimensions of the feature set while retaining 
the information that discriminates the output class label as well.

-- LDA tries to find a decision boundary around each cluster of a class 

-- It will then project these data points in a new dimension such that
all the clusters are separate from each other as much as possible. 
hence, the individual points in a cluster are closer to the centroid of 
that particular cluster.

-- These new dimensions form the linear discriminants of the feature set.

-- LDA also works well on a normalized dataset

-----------------------------------------------------------------------

Choose PCA --> When the data is highly irregular in terms of 
               distribution (skewed)
Choose LDA --> Uniform distribution 

--------------------------------------------------------------------------
Reference:

https://builtin.com/data-science/step-step-explanation-principal-component-analysis

------------------------------------------------------------------------------
Dimensionality reduction in Regression:

House price :

Chennai city at 5 different areas and each area had 2 unique builders 

House price --> target 

Area name , builder name , sqft , balcony , no of bathrooms, no of bedrooms
presence of a park nearby , hospital , car parking , water availability

==> we let go of sqft --> we will be leftout with 9 columns 

area name , no of bathrooms, no of bedrooms ==> model 


------------------------------------------------------------------------------
why standardization? 

employee dataset 

age -> 20 - 65
salary -> 20,000 -  3l / month
years of exp -> 0 - 30 

y--> hike (in terms of rupees)  -- > emp1 -> 10,500 
                                     emp2 -> 8,990

ypes of recommender systems:
-----------------------------
1. Personalized - based on user profile and contextual parameters
2. Collaborative - recommends items popular among the community
3. Content Based - displays similar to what you like
4. Hybrid - A combination of all the above 

User based Collaborative Filtering : In this model, the products are 
recommended to a particular user based on the fact that the products have been
liked by users who have similar interests as the user to whom the product is
intended to be recommended.

Eg: Amazon Prime Video

Vaishali - Sci fi , horror, romance | User based CF would recommend Comedy
Vignesh - Comedy, Action | User based CF would recommend Sci-fi
Nithin - Sci-fi , romance | User based CF would recommend horror
Naveena - Comedy, Sci-fi | User based CF would recommend Action,horror , romance

Item based CF : These are systems that identify similar items based on the 
ratings given by multiple users previously for those items.

For eg: Let's say that 3 customers A, B & C have given a  5 star rating for 
2 books - X & Y. Now, a new customer D buys the book Y. 

Now, in an Item Based CF system, the book X will also be recommended to 
user D because both the books X & Y have similar ratings. 

Scenario to understand Item based CF:

book store - 10 different book genres

cust A - g1,g2,g3 , assuming that cust A gives good rating for g2&g3 books, cust B is recommended with g3 book
cust B - g2,g4,g5 , assuming that cust B gives good rating for g2&g4 books, cust A is recommended with g4 book
cust C - g8,g9,g10  --> general recommendations like top selling books 
==============================================================================================












































































